import sys
import os
import time
import logging
import threading
from pathlib import Path
import numpy as np
import torch
from asr.common import TTSClient, AgentClient, setup_logger
from asr.interaction.utils.buffer import recognition_buffer
from asr.interaction.utils.audio import get_audio_device, get_audio_config, create_input_stream
from asr.interaction.utils.wake_word import check_wake_word
from asr.interaction.context import set_system

# é…ç½®æ—¥å¿—
logger = setup_logger(__name__)

# å¿…é¡»ç¡®ä¿ sys.path å·²ç”±å…¥å£è„šæœ¬è®¾ç½®å¥½ï¼Œæ‰èƒ½å¯¼å…¥ä»¥ä¸‹æ¨¡å—
try:
    from asr.streaming_sensevoice_master.streaming_sensevoice import StreamingSenseVoice
    from pysilero import VADIterator
    from pypinyin import lazy_pinyin
except ImportError:
    pass # ç”±ä¸»ç¨‹åºå¤„ç†

class InteractionSystem:
    # çŠ¶æ€å®šä¹‰
    STATE_WAIT_WAKE = "WAIT_WAKE"   # ç­‰å¾…å”¤é†’
    STATE_LISTENING = "LISTENING"   # æ­£åœ¨å€¾å¬ç”¨æˆ·æŒ‡ä»¤
    STATE_THINKING = "THINKING"     # è°ƒç”¨ Agent æ€è€ƒä¸­
    STATE_SPEAKING = "SPEAKING"     # TTS æ’­æŠ¥ä¸­

    def __init__(self):
        # æ³¨å†Œè‡ªèº«åˆ°å…¨å±€ä¸Šä¸‹æ–‡
        set_system(self)
        
        self.wake_word = "å°å®‰"
        self.wake_word_pinyin = lazy_pinyin(self.wake_word) if lazy_pinyin else None
        
        self.state = self.STATE_WAIT_WAKE
        self.is_running = True
        
        # å”¤é†’æ§åˆ¶
        self.wake_detection_paused = False
        
        # VAD & è¯†åˆ«çŠ¶æ€
        self.is_speech_active = False
        self.last_speech_time = 0
        self.current_text_buffer = ""
        
        # 1. åˆå§‹åŒ–æ¨¡å‹
        self._init_model()
        
        # 2. åˆå§‹åŒ–å®¢æˆ·ç«¯
        self.agent = AgentClient()
        
        # 3. åˆå§‹åŒ– VAD
        self.vad = VADIterator(min_silence_duration_ms=1000, speech_pad_ms=100)
        
        # 4. å”¤é†’æš‚åœæ§åˆ¶
        self.pause_source = None
        self.pause_lock = threading.Lock()
        
        print(f"âœ… ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ (å”¤é†’è¯: {self.wake_word})")

    def _init_model(self):
        device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"ğŸ–¥ï¸  è¿è¡Œè®¾å¤‡: {device.upper()}")
        
        # æœ¬åœ°å¾®è°ƒæ¨¡å‹è·¯å¾„
        local_model_dir = "/home/devuser/workspace/asr/FunASR-main/examples/industrial_data_pretraining/sense_voice/outputs/sensevoice_finetune_v3"
        model_id = local_model_dir if os.path.exists(local_model_dir) else "iic/SenseVoiceSmall"
        
        print(f"æ­£åœ¨åŠ è½½ StreamingSenseVoice æ¨¡å‹: {model_id}")
        contexts = [self.wake_word, "å˜"]
        
        self.model = StreamingSenseVoice(
            contexts=contexts,
            model=model_id,
            device=device,
        )
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")

    def set_wake_paused(self, paused: bool):
        """(å·²å¼ƒç”¨) è¯·ä½¿ç”¨ pause_wake_detection / resume_wake_detection"""
        self.wake_detection_paused = paused
        if paused:
            self.model.reset()
            self.current_text_buffer = ""

    def pause_wake_detection(self, source: str) -> bool:
        """æš‚åœå”¤é†’æ£€æµ‹ (å¸¦æ¥æºè®°å½•)"""
        with self.pause_lock:
            if self.wake_detection_paused:
                # å·²ç»è¢«æš‚åœäº†
                if self.pause_source == source:
                    return True # åŒä¸€ä¸ªæºï¼Œè§†ä¸ºæˆåŠŸ
                else:
                    print(f"âš ï¸ æš‚åœå¤±è´¥: å·²è¢« '{self.pause_source}' æš‚åœ")
                    return False
            
            self.wake_detection_paused = True
            self.pause_source = source
            # é‡ç½®çŠ¶æ€
            self.model.reset()
            self.current_text_buffer = ""
            return True

    def resume_wake_detection(self, source: str) -> bool:
        """æ¢å¤å”¤é†’æ£€æµ‹ (å¸¦æ¥æºéªŒè¯)"""
        with self.pause_lock:
            if not self.wake_detection_paused:
                return True # æœ¬æ¥å°±æ²¡æš‚åœ
            
            if self.pause_source != source:
                print(f"âš ï¸ æ¢å¤å¤±è´¥: å½“å‰ç”± '{self.pause_source}' æš‚åœ, '{source}' æ— æƒæ¢å¤")
                return False
            
            self.wake_detection_paused = False
            self.pause_source = None
            return True

    def handle_wake_up(self):
        """å¤„ç†å”¤é†’äº‹ä»¶"""
        print("ğŸ’¡ è§¦å‘å”¤é†’é€»è¾‘...")
        
        # 1. åˆ‡æ¢çŠ¶æ€ (å…ˆè®¾ä¸º SPEAKING ä»¥å¿½ç•¥ "æˆ‘åœ¨" çš„å£°éŸ³)
        self.state = self.STATE_SPEAKING
        
        # 2. å¯åŠ¨äº¤äº’çº¿ç¨‹ï¼Œé¿å…é˜»å¡ä¸»å¾ªç¯éŸ³é¢‘è¯»å–
        threading.Thread(target=self._run_interaction, daemon=True).start()

    def _run_interaction(self):
        """äº¤äº’æµç¨‹æ‰§è¡Œçº¿ç¨‹ (æ”¯æŒè¿ç»­å¯¹è¯)"""
        try:
            # 1. å”¤é†’åç«‹å³ç”³è¯·ç‹¬å æƒï¼Œç›´åˆ°å¯¹è¯å½»åº•ç»“æŸæ‰é‡Šæ”¾
            TTSClient.set_exclusive_mode(True, allowed_source="interaction")
            
            # ğŸ†• é‡ç½® Agent ä¼šè¯ (Session ID)ï¼Œå¼€å¯æ–°çš„å¯¹è¯ä¸Šä¸‹æ–‡
            # è¿™æ ·å¯ä»¥ç¡®ä¿æ¯æ¬¡å”¤é†’éƒ½æ˜¯ä¸€æ¬¡å…¨æ–°çš„å¯¹è¯ï¼Œåªæœ‰åœ¨æœ¬æ¬¡è¿ç»­äº¤äº’ä¸­æ‰ä¿ç•™è®°å¿†
            self.agent.reset_session()
            
            # æ’­æ”¾å”¤é†’éŸ³æ•ˆ/è¯­éŸ³
            TTSClient.speak("æˆ‘åœ¨", wait=True, source="interaction")
            # time.sleep(0.2) # âš¡ ä¼˜åŒ–: ç§»é™¤é¢å¤–ç­‰å¾…ï¼ŒåŠ é€Ÿè¿›å…¥ç›‘å¬çŠ¶æ€
            
            # min_silence_duration_ms : å†³å®šäº† â€œç­‰å¤šä¹…æ‰ç®—å®Œâ€
            # speech_pad_ms : å†³å®šäº† â€œå¤šä¿ç•™å¤šå°‘å£°éŸ³â€
            # ğŸ†• ä¼˜åŒ–: å°† speech_pad_ms ä» 1500ms é™ä½åˆ° 500msï¼Œå‡å°‘éŸ³é¢‘é‡å å¯¼è‡´çš„"å˜å˜"é‡å¤é—®é¢˜
            self.vad = VADIterator(min_silence_duration_ms=2000, speech_pad_ms=200)
            
            self.state = self.STATE_LISTENING
            
            # è¿›å…¥è¿ç»­å¯¹è¯å¾ªç¯
            while True:
                should_continue = self._process_one_turn()
                if not should_continue:
                    break
                # æ¯ä¸€è½®ç»“æŸåï¼Œç¨å¾®ç­‰å¾…ä¸€ä¸‹
                time.sleep(0.1)
                
        except Exception as e:
            print(f"âŒ äº¤äº’å¾ªç¯å¼‚å¸¸: {e}")
        finally:
            # é€€å‡ºäº¤äº’ï¼Œé‡ç½®çŠ¶æ€
            self.state = self.STATE_WAIT_WAKE
            self.is_speech_active = False
            self.model.reset()
            # æ¢å¤é»˜è®¤ VAD è®¾ç½®
            self.vad = VADIterator(min_silence_duration_ms=1000, speech_pad_ms=100)
            
            # ç¡®ä¿é‡Šæ”¾ç‹¬å æƒ
            TTSClient.set_exclusive_mode(False, allowed_source="interaction")
            print("ğŸ’¤ å›åˆ°ç­‰å¾…å”¤é†’æ¨¡å¼")

    def _process_one_turn(self) -> bool:
        """å¤„ç†ä¸€è½®å¯¹è¯ï¼Œè¿”å›æ˜¯å¦ç»§ç»­"""
        print("\nğŸ¤ è¯·è¯´è¯...")
        
        # å½•éŸ³å‚æ•°
        listen_duration = 8.0  # æœ€å¤§è†å¬æ—¶é—´
        silence_timeout = 2.0  # æ²‰é»˜è¶…æ—¶
        
        recognition_buffer.start_recording()
        
        final_query = ""
        try:
            start_time = time.time()
            last_speech_end = time.time()
            has_spoken = False
            
            while time.time() - start_time < listen_duration:
                # æ£€æŸ¥æ˜¯å¦è¶…æ—¶ï¼ˆè¯´å®Œåæ²‰é»˜äº†ä¸€æ®µæ—¶é—´ï¼‰
                if has_spoken and (time.time() - last_speech_end > silence_timeout):
                    print("âš¡ è¯´è¯ç»“æŸåˆ¤å®š")
                    break
                
                # æ›´æ–°è¯´è¯çŠ¶æ€
                if self.is_speech_active:
                    has_spoken = True
                    last_speech_end = time.time()
                    # å¦‚æœæ­£åœ¨è¯´è¯ï¼Œå»¶é•¿æ€»è†å¬æ—¶é—´
                    if time.time() - start_time > listen_duration - 2.0:
                        listen_duration += 1.0
                
                time.sleep(0.1)
            
            # è·å–è¯†åˆ«ç»“æœ
            # æ³¨æ„: åœ¨ start_recording() çŠ¶æ€ä¸‹ï¼Œget_recent ä¼šè‡ªåŠ¨è·å–ä»å½•éŸ³å¼€å§‹åˆ°ç°åœ¨çš„æ‰€æœ‰å†…å®¹ï¼Œduration å‚æ•°ä¼šè¢«å¿½ç•¥
            final_query = recognition_buffer.get_recent()
            print(f"\nğŸ“ è¯†åˆ«ç»“æœ: {final_query}")
            
        finally:
            recognition_buffer.stop_recording()

        # 1. è¶…æ—¶æ£€æµ‹ (æ— è¯­éŸ³)
        if not final_query:
            print("âŒ› äº¤äº’è¶…æ—¶ (æ— è¯­éŸ³)")
            self.state = self.STATE_SPEAKING
            TTSClient.speak("å†è§", wait=True, source="interaction")
            return False

        # 2. é€€å‡ºæŒ‡ä»¤æ£€æµ‹
        exit_keywords = ["ç»“æŸå¯¹è¯", "é€€å‡º", "åœæ­¢äº¤äº’", "å…³é—­å¯¹è¯", "å†è§", "ç»“æŸ"]
        if any(kw in final_query for kw in exit_keywords):
            print(f"ğŸ›‘ ç”¨æˆ·è¯·æ±‚é€€å‡º: {final_query}")
            self.state = self.STATE_SPEAKING # ğŸ†• é˜²æ­¢å¬åˆ°è‡ªå·±çš„å£°éŸ³ (å›å£°æ¶ˆé™¤)
            TTSClient.speak("å¥½çš„ï¼Œå†è§", wait=True, source="interaction")
            return False

        # 3. Agent äº¤äº’
        self.state = self.STATE_THINKING
        try:
            response = self.agent.chat(final_query)
            print(f"ğŸ¤– Agent: {response}")
            # TODO å›ç­”å¤„ç†æ¨¡å—
            # è¿›å…¥æ’­æŠ¥æ¨¡å¼
            self.state = self.STATE_SPEAKING
            
            # ç›´æ¥æ’­æŠ¥ (ç‹¬å æƒå·²åœ¨ _run_interaction ç»Ÿä¸€ç®¡ç†)
            TTSClient.speak(response, wait=True, source="interaction")
            # TODO æ ¹æ®è¯†åˆ«åˆ°çš„è¯­éŸ³å¢åŠ  æ’­æ”¾æš‚åœæ¨¡å—
            time.sleep(0.5) # ç­‰å¾…å°¾éŸ³ç»“æŸ
                    
        except Exception as e:
            print(f"âŒ äº¤äº’å¼‚å¸¸: {e}")
            TTSClient.speak("æˆ‘å‡ºé”™äº†", wait=True, source="interaction")
        
        # å‡†å¤‡ä¸‹ä¸€è½®ï¼Œåˆ‡æ¢å›ç›‘å¬çŠ¶æ€
        self.state = self.STATE_LISTENING
        return True

    def run(self):
        # 1. è·å–éŸ³é¢‘è®¾å¤‡
        target_device_idx = get_audio_device("Newmine Mic")

        # 2. è·å–éŸ³é¢‘é…ç½®
        target_sample_rate = 16000
        chunk_duration = 0.1
        
        stream_sample_rate, samples_per_read, use_resample, resampler = get_audio_config(
            target_device_idx, 
            target_sample_rate, 
            chunk_duration
        )
        
        if use_resample and not resampler:
             from scipy import signal

        self.current_text_buffer = "" 
        self.is_speech_active = False 
        
        stream = create_input_stream(target_device_idx, stream_sample_rate)
        stream.start()
        print(f"\nğŸš€ ç³»ç»Ÿå°±ç»ª,è¯·è¯´ '{self.wake_word}' å”¤é†’")

        try:
            while True:
                # ç»Ÿä¸€è¯»å–éŸ³é¢‘
                samples, _ = stream.read(samples_per_read)
                audio_chunk = samples[:, 0]
                
                if use_resample:
                    if resampler:
                        audio_chunk = resampler.resample_chunk(audio_chunk)
                    else:
                        num_output = int(len(audio_chunk) * target_sample_rate / stream_sample_rate)
                        audio_chunk = signal.resample(audio_chunk, num_output)
                
                if self.state == self.STATE_WAIT_WAKE:
                    # ===== ç­‰å¾…å”¤é†’æ¨¡å¼ =====
                    # å³ä½¿æš‚åœå”¤é†’ï¼Œä¹Ÿè¦ç»§ç»­å¤„ç†éŸ³é¢‘ä»¥æ›´æ–° Bufferï¼Œä¾›æ—è·¯ç›‘å¬ä½¿ç”¨
                    # ä½†åœ¨æš‚åœæœŸé—´ï¼Œä¸è¿›è¡Œå”¤é†’è¯åŒ¹é…
                    
                    vad_outs = self.vad(audio_chunk)
                        
                    for speech_dict, speech_samples in vad_outs:
                        if "start" in speech_dict:
                            self.is_speech_active = True
                            self.model.reset()
                            self.current_text_buffer = ""
                            self.last_speech_time = time.time()
                        
                        text = ""
                        for res in self.model.streaming_inference(speech_samples * 32768, "end" in speech_dict):
                            text = res.get("text", "")
                            if text:
                                if len(text) < 2 or len(set(text)) == 1:
                                    continue
                            
                                if text != self.current_text_buffer:
                                    print(f"\rğŸ‘‚ è¯†åˆ«ä¸­: {text}", end="", flush=True)
                                    self.current_text_buffer = text
                                    recognition_buffer.add(text)
                    
                        # åªæœ‰åœ¨æœªæš‚åœå”¤é†’æ£€æµ‹æ—¶ï¼Œæ‰æ£€æŸ¥å”¤é†’è¯
                        if not self.wake_detection_paused and text and check_wake_word(text, self.wake_word, self.wake_word_pinyin):
                            if not recognition_buffer.is_active:
                                print(f"\nğŸš€ æ£€æµ‹åˆ°å”¤é†’è¯ï¼")
                                self.handle_wake_up()
                                self.current_text_buffer = ""
                                self.model.reset()
                                break
                            else:
                                print(f"\rğŸ‘‚ è¯†åˆ«ä¸­: {text} (å¤–éƒ¨å½•éŸ³ä¸­,æš‚ä¸å“åº”å”¤é†’)", end="", flush=True)
                        elif not recognition_buffer.is_active and "end" in speech_dict:
                            self.model.reset()
                            self.current_text_buffer = ""
                else:
                    # ===== äº¤äº’æ¨¡å¼ =====
                    # å¦‚æœæ­£åœ¨æ€è€ƒæˆ–æ’­æŠ¥ï¼Œæš‚åœè¯†åˆ«ä»¥é¿å…è‡ªå›å£°
                    if self.state in [self.STATE_THINKING, self.STATE_SPEAKING]:
                        time.sleep(0.01)
                        continue

                    # VAD ä»ç„¶è¿è¡Œä»¥æ£€æµ‹è¯´è¯ç»“æŸ
                    vad_outs = self.vad(audio_chunk)
                    for speech_dict, speech_samples in vad_outs:
                        if "start" in speech_dict:
                            self.is_speech_active = True
                            self.model.reset() # ğŸ†• ä¿®å¤: æ–°çš„ä¸€å¥å¼€å§‹æ—¶ï¼Œå¿…é¡»é‡ç½®æ¨¡å‹çŠ¶æ€
                            self.last_speech_time = time.time()
                        if "end" in speech_dict:
                            self.is_speech_active = False
                            self.last_speech_time = time.time()
                        
                        for res in self.model.streaming_inference(speech_samples * 32768, "end" in speech_dict):
                            text = res.get("text", "")
                            if text and text != self.current_text_buffer:
                                print(f"\rğŸ¤ äº¤äº’è¯†åˆ«: {text}", end="", flush=True)
                                self.current_text_buffer = text
                                recognition_buffer.add(text)
                
                time.sleep(0.001)

        except KeyboardInterrupt:
            print("\nğŸ›‘ åœæ­¢æœåŠ¡...")
            stream.stop()
            stream.close()
            # ğŸ†• ä»…åœ¨éç­‰å¾…å”¤é†’çŠ¶æ€ä¸‹ï¼ˆå³å¯èƒ½æŒæœ‰é”çš„çŠ¶æ€ï¼‰æ‰å°è¯•é‡Šæ”¾
            if self.state != self.STATE_WAIT_WAKE:
                TTSClient.set_exclusive_mode(False, allowed_source="interaction")
